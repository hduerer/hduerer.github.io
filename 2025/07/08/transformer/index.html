<!DOCTYPE html>
<html lang="zh">
  <head>
    
    <meta charset="UTF-8">
    <title>Transformer学习笔记 - Hduer</title>
    <link rel="shortcut icon" href="/static/img/icon.png">
    <link rel="icon" href="/static/img/icon.png" sizes="192x192"/>
    
<link rel="stylesheet" href="/static/kico.css">
<link rel="stylesheet" href="/static/hingle.css">

    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <meta property="og:site_name" content="Hduer">
    <meta property="og:title" content="Transformer学习笔记"/>
    
<meta name="generator" content="Hexo 7.3.0"></head>

  <body>
    <header>
    <div class="head-title">
        <h4>Hduer</h4>
    </div>
    <div class="head-action">
        <div class="toggle-btn"></div>
        <div class="light-btn"></div>
        <div class="search-btn"></div>
    </div>
    <form class="head-search" method="post">
        <input type="text" name="s" placeholder="搜索什么？">
    </form>
    <nav class="head-menu">
        <a href="/">首页</a>
        <div class="has-child">
            <a href>分类</a>
            <div class="sub-menu">
                <a class="category-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a><a class="category-link" href="/categories/%E8%BF%90%E7%BB%B4/">运维</a>
            </div>
        </div>
        
            <a href="/about">关于我</a>
        
            <a href="/friends">朋友们</a>
        
    </nav>
</header>

    <main>
    <div class="wrap min">
        <section class="post-title">
            <h2>Transformer学习笔记</h2>
            <div class="post-meta">
                <time class="date">2025.07.08</time>
            
                <span class="category"><a class="category-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span>
            
            </div>
        </section>
        <article class="post-content">
        
            <h2 id="一-CNN-卷积神经网络"><a href="#一-CNN-卷积神经网络" class="headerlink" title="一. CNN (卷积神经网络)"></a>一. CNN (卷积神经网络)</h2><h3 id="1-1-CNN出现的背景"><a href="#1-1-CNN出现的背景" class="headerlink" title="1.1 CNN出现的背景"></a>1.1 CNN出现的背景</h3><p>在深度学习出现之前，图像识别、目标检测等视觉任务通常依赖于人工设计的特征（如 SIFT、HOG），模型的泛化能力有限，且不易迁移。</p>
<p>随着深度学习的发展，研究者发现神经网络可以自动从原始像素中学习特征。但传统的全连接网络在处理高维图像时存在以下问题：</p>
<ol>
<li>参数量巨大（全连接导致维度爆炸）；</li>
<li>缺乏空间结构建模能力（图像的局部性和位置信息无法利用）；</li>
<li>训练不稳定，泛化差。</li>
</ol>
<p>为了解决这些问题，<strong>Yann LeCun</strong> 等人在 1989 年提出 LeNet 系列模型，使用卷积（Convolution）和池化（Pooling）提取局部特征。随后 AlexNet 在 2012 年 ImageNet 比赛中取得巨大突破，引发了深度学习浪潮。</p>
<p>CNN 通过 <strong>局部连接、权重共享、下采样</strong> 等机制，有效捕捉图像中的局部结构，是计算机视觉中最核心的网络之一。</p>
<h3 id="1-2-CNN的整体架构"><a href="#1-2-CNN的整体架构" class="headerlink" title="1.2 CNN的整体架构"></a>1.2 CNN的整体架构</h3><p>CNN由输入层、卷积层、激活函数、池化层以及全连接层构成。</p>
<p>INPUT（输入层）-CONV（卷积层）-RELU（激活函数）-POOL（池化层）-FC（全连接层）</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20250706232828.png" alt="微信图片_20250706232828"></p>
<p>卷积操作可参考以下图片:</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20250707004750.png" alt="微信图片_20250707004750"></p>
<p>整个流程可参考以下过程:</p>
<p>我们假设输入图像为：</p>
<ul>
<li><p>尺寸：<code>32×32</code> 像素(可理解为32*32的矩阵)</p>
</li>
<li><p>通道：RGB（一般彩色图像由RGB三通道组成）</p>
</li>
<li><p>类别数：10（用于图像分类，如 CIFAR-10）</p>
</li>
</ul>
<h4 id="输入层处理"><a href="#输入层处理" class="headerlink" title="输入层处理:"></a><strong>输入层处理:</strong></h4><ul>
<li>输入张量大小：<code>[batch_size, 3, 32, 32]</code></li>
<li>表示：<code>batch_size</code> 张图片，每张有 3 个通道，32×32 像素</li>
</ul>
<h4 id="卷积层1处理"><a href="#卷积层1处理" class="headerlink" title="卷积层1处理:"></a>卷积层1处理:</h4><ul>
<li>卷积核数目：16</li>
<li>卷积核大小：3×3</li>
<li>步幅：1</li>
<li>Padding：1(可理解为填充处理,原矩阵四周用0填充,此操作为了使得经过卷积后的矩阵和原图像的矩阵大小相同。本图像<strong>32*32</strong>的矩阵经填充得到<strong>34*34</strong>的矩阵,再经卷积操作得到<strong>32*32</strong>的矩阵)</li>
<li>输出大小：<code>[batch_size, 16, 32, 32]</code>(16是因为有16个卷积核)</li>
</ul>
<h4 id="激活函数处理"><a href="#激活函数处理" class="headerlink" title="激活函数处理:"></a>激活函数处理:</h4><p>ReLU：将负数置 0，保持非线性特征</p>
<h4 id="池化层1处理"><a href="#池化层1处理" class="headerlink" title="池化层1处理:"></a>池化层1处理:</h4><p>最大池化（MaxPooling）</p>
<p>池化窗口：2×2</p>
<p>步幅：2</p>
<p>输出大小：<code>[batch_size, 16, 16, 16]</code>(将<strong>32*32</strong>的矩阵分成16个<strong>2*2</strong>矩阵的区域,并取每个区域的最大值,得到<strong>16*16</strong>的矩阵)</p>
<h4 id="卷积层2处理"><a href="#卷积层2处理" class="headerlink" title="卷积层2处理:"></a>卷积层2处理:</h4><ul>
<li>卷积核数目：32</li>
<li>卷积核大小：3×3</li>
<li>Padding：1</li>
<li>输出大小：<code>[batch_size, 32, 16, 16]</code></li>
</ul>
<h4 id="激活函数处理-1"><a href="#激活函数处理-1" class="headerlink" title="激活函数处理"></a>激活函数处理</h4><h4 id="池化层2处理"><a href="#池化层2处理" class="headerlink" title="池化层2处理:"></a>池化层2处理:</h4><ul>
<li>最大池化</li>
<li>输出大小：<code>[batch_size, 32, 8, 8]</code></li>
</ul>
<h4 id="Flatten-展平层处理"><a href="#Flatten-展平层处理" class="headerlink" title="Flatten 展平层处理:"></a><strong>Flatten 展平层处理:</strong></h4><ul>
<li>把 <code>[32, 8, 8]</code> 展平成 <code>[32×8×8 = 2048]</code> 的向量</li>
</ul>
<h4 id="全连接层处理"><a href="#全连接层处理" class="headerlink" title="全连接层处理:"></a>全连接层处理:</h4><ul>
<li>输入：2048</li>
<li>输出：128（中间隐藏层, 这是我们人为设定的中间维度，用于学习抽象特征）</li>
</ul>
<h4 id="激活函数处理-2"><a href="#激活函数处理-2" class="headerlink" title="激活函数处理"></a>激活函数处理</h4><h4 id="输出层处理"><a href="#输出层处理" class="headerlink" title="输出层处理:"></a><strong>输出层</strong>处理:</h4><ul>
<li>输出：10（输入的类别数为10）</li>
<li>通常接 Softmax（训练时可用 CrossEntropyLoss）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 输入：3通道，输出：16通道，卷积核3x3，padding=1保证尺寸不变</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 第二个卷积层：输入16通道，输出32通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=<span class="number">16</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 全连接层：输入是池化后的特征图展平后的长度（32 * 8 * 8）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)  <span class="comment"># 输出10类</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 卷积 → ReLU → 池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))     <span class="comment"># [B, 16, 32, 32]</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)        <span class="comment"># [B, 16, 16, 16]</span></span><br><span class="line">        </span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))     <span class="comment"># [B, 32, 16, 16]</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)        <span class="comment"># [B, 32, 8, 8]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 展平</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)     <span class="comment"># [B, 32*8*8 = 2048]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))       <span class="comment"># [B, 128]</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)               <span class="comment"># [B, 10]</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="1-3-CNN的优势"><a href="#1-3-CNN的优势" class="headerlink" title="1.3 CNN的优势"></a>1.3 CNN的优势</h3><table>
<thead>
<tr>
<th>优势</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>局部连接</td>
<td>每个神经元只关注图像的局部区域，类似人类视觉</td>
</tr>
<tr>
<td>权重共享</td>
<td>一个卷积核在整个图像中滑动，减少参数，提高泛化能力</td>
</tr>
<tr>
<td>空间不变性</td>
<td>平移、缩放等变化对识别结果影响小</td>
</tr>
<tr>
<td>可堆叠性强</td>
<td>多层卷积可以提取从边缘、纹理到语义的层次特征</td>
</tr>
</tbody></table>
<h3 id="1-4-CNN存在的问题"><a href="#1-4-CNN存在的问题" class="headerlink" title="1.4 CNN存在的问题"></a>1.4 CNN存在的问题</h3><table>
<thead>
<tr>
<th>问题</th>
<th>描述</th>
<th>常见应对方法</th>
</tr>
</thead>
<tbody><tr>
<td>参数依然多</td>
<td>高分辨率图像会产生大量卷积</td>
<td>使用小卷积核 + 池化层</td>
</tr>
<tr>
<td>无法捕捉长距离依赖</td>
<td>感受野有限</td>
<td>使用更深层或结合 Transformer</td>
</tr>
<tr>
<td>对位置不敏感</td>
<td>位置信息可能丢失</td>
<td>使用 CoordConv、位置编码</td>
</tr>
<tr>
<td>对时间维无效</td>
<td>不能处理音频&#x2F;文本等序列数据</td>
<td>联合 RNN &#x2F; Transformer 使用</td>
</tr>
</tbody></table>
<h3 id="1-5-CNN在当下的应用"><a href="#1-5-CNN在当下的应用" class="headerlink" title="1.5 CNN在当下的应用"></a>1.5 CNN在当下的应用</h3><p>CNN 是专门为处理具有<strong>网格结构数据（如图像）而设计的神经网络，目前依然是计算机视觉领域</strong>的基础架构之一。</p>
<h4 id="1-5-1-图像领域"><a href="#1-5-1-图像领域" class="headerlink" title="1.5.1 图像领域"></a>1.5.1 图像领域</h4><ul>
<li><strong>图像分类</strong>：如医疗图像中的疾病识别（X光、CT）、卫星图像识别等。</li>
<li><strong>目标检测</strong>：YOLO、Faster R-CNN 等架构仍以 CNN 为基础，用于自动驾驶、安防监控。</li>
<li><strong>图像分割</strong>：如语义分割（分辨图像中每个像素的类别），常用于医学图像、自动驾驶。</li>
<li><strong>图像生成&#x2F;超分辨率</strong>：SRGAN、ESRGAN 这类生成网络也以 CNN 为核心。</li>
<li><strong>人脸识别</strong>：虽然如今很多系统采用 Transformer，但轻量化场景（如移动端）仍偏向 CNN。</li>
</ul>
<h4 id="1-5-2-视频分析"><a href="#1-5-2-视频分析" class="headerlink" title="1.5.2 视频分析"></a>1.5.2 视频分析</h4><ul>
<li><strong>动作识别</strong>：如视频中的行为分析、体育比赛中的战术分析。</li>
<li><strong>视频摘要&#x2F;分镜提取</strong>：CNN+LSTM&#x2F;GRU 或 CNN+Transformer 的混合结构仍然常见。</li>
</ul>
<h4 id="1-5-3-其他领域"><a href="#1-5-3-其他领域" class="headerlink" title="1.5.3 其他领域"></a>1.5.3 其他领域</h4><ul>
<li><strong>边缘计算&#x2F;嵌入式设备</strong>：因为 CNN 计算开销较小，仍在移动端&#x2F;IoT 等设备中大量使用。</li>
<li><strong>工业质检</strong>：使用 CNN 模型分析产品图像找缺陷。</li>
</ul>
<h2 id="二-RNN-循环神经网络"><a href="#二-RNN-循环神经网络" class="headerlink" title="二. RNN (循环神经网络)"></a>二. RNN (循环神经网络)</h2><h3 id="2-1-RNN出现的背景"><a href="#2-1-RNN出现的背景" class="headerlink" title="2.1 RNN出现的背景"></a>2.1 RNN出现的背景</h3><p>在 NLP、语音识别等任务中，输入数据是有时间顺序的序列，而传统神经网络（如全连接网络、CNN）不具备建模序列关系的能力。</p>
<p>为此，<strong>RNN（Recurrent Neural Network）</strong> 被提出，用于处理 <strong>序列数据</strong>，其核心思想是：每个时刻的输出依赖于上一个时刻的状态，从而实现信息的“记忆”与“传递”。</p>
<p>然而，原始的 RNN 存在 <strong>梯度消失&#x2F;爆炸问题</strong>，导致模型难以捕捉长期依赖关系。为了解决这一问题，后续提出了改进版本：</p>
<ul>
<li><strong>LSTM（Long Short-Term Memory）</strong>：引入门控机制，控制信息流；</li>
<li><strong>GRU（Gated Recurrent Unit）</strong>：结构更简化，效果与 LSTM 相近。</li>
</ul>
<p>RNN 在 2010 年代广泛应用于 <strong>语言模型、机器翻译、语音识别、时间序列预测</strong> 等任务。</p>
<h3 id="2-2-RNN的整体架构"><a href="#2-2-RNN的整体架构" class="headerlink" title="2.2 RNN的整体架构"></a>2.2 RNN的整体架构</h3><p><img src="/images/image-20250707010801726.png" alt="image-20250707010801726"></p>
<p>基本的RNN架构:</p>
<p><img src="/images/image-20250707010245974.png" alt="image-20250707010245974"></p>
<p>LSTM架构:</p>
<p><img src="/images/image-20250707025423398.png" alt="image-20250707025423398"></p>
<p> GRU架构:</p>
<p><img src="/images/image-20250707025556172.png" alt="image-20250707025556172"></p>
<p>以基础的RNN架构为例:</p>
<p>我们假设要处理一段文本，判断它是“正面”还是“负面”情感，即<strong>二分类任务</strong>。我们用 RNN 来处理这个序列任务。</p>
<p>假设有以下句子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;I love this movie&quot;</span><br></pre></td></tr></table></figure>

<p>假设词汇表大小为 <code>vocab_size=10000</code>，每个词编码为整数。我们用词嵌入（Embedding）将这些词转换为向量。</p>
<ul>
<li><strong>句子长度</strong>：5（“I love this movie <PAD>”）补齐到 5 (RNN以及大多数深度学习模型<strong>需要输入的张量尺寸统一</strong>，否则无法批量处理数据。填充的 <code>&lt;PAD&gt;</code> 会有专门的索引（如 0）。在更高级模型中比如 LSTM、Transformer，通常会用 <strong>mask</strong> 来忽略掉 <code>&lt;PAD&gt;</code> 的影响。)</li>
<li><strong>每个词向量维度</strong>：<code>embedding_dim=64</code>(作用是把每个词从一个整数 ID 转换为一个向量。更高的维度意味着更丰富的语义表示能力)。</li>
<li><strong>RNN 隐藏层大小</strong>：<code>hidden_size=128</code>(该值表示 RNN 每一步存储的“记忆”大小。值越大，RNN 的表达能力越强，但计算量也更大)</li>
<li><strong>输出类别数</strong>：2（正&#x2F;负）</li>
</ul>
<h4 id="输入层处理-1"><a href="#输入层处理-1" class="headerlink" title="输入层处理:"></a>输入层处理:</h4><p>输入：形状为 <code>[batch_size, seq_len]</code> 的整数张量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 假设每个词已经转为整数编码</span><br><span class="line">x = [[12, 45, 98, 256, 0]]  # 假设 batch_size = 1，0 是 &lt;PAD&gt;</span><br></pre></td></tr></table></figure>



<h4 id="嵌入层处理"><a href="#嵌入层处理" class="headerlink" title="嵌入层处理:"></a>嵌入层处理:</h4><p>把每个词编码映射为一个向量</p>
<p>输入张量的形状：<code>[batch_size, seq_len]</code>即<code>[1, 5]</code> → 输出张量的形状：<code>[1, 5, 64]</code></p>
<p>此时每个词变成 64 维向量</p>
<h4 id="RNN-层（或-LSTM-GRU）-处理"><a href="#RNN-层（或-LSTM-GRU）-处理" class="headerlink" title="**RNN 层（或 LSTM&#x2F;GRU）**处理"></a>**RNN 层（或 LSTM&#x2F;GRU）**处理</h4><ul>
<li>接收张量的形状:<code>[batch_size, seq_len, embedding_dim]</code>即<code>[1, 5, 64]</code></li>
<li>输出两个结果：<ul>
<li><code>output</code>即<code>[batch_size, seq_len, hidden_size]</code>：表示每个时间步的隐藏状态,共有 5 个时间步，每步都有一个 128维状态 → <code>[1, 5, 128]</code></li>
<li><code>hidden</code>：表示最后时间步的隐藏状态(总结这句话的记忆) → <code>[1, 1，128]</code>（用于分类）</li>
</ul>
</li>
</ul>
<h4 id="全连接层处理-1"><a href="#全连接层处理-1" class="headerlink" title="全连接层处理"></a><strong>全连接层</strong>处理</h4><ul>
<li>输入：<code>hidden</code>（最后时间步的隐藏状态）</li>
<li>输出张量的形状:<code>[batch_size,类别数 ]</code>：<code>[1, 2]</code></li>
</ul>
<p>如可能最终输出<code>[1.7, -0.3]</code>,这是模型实际输出的 <strong>数值内容</strong>，即 logits, 表示模型对两个类别的打分值, 在此之后经过 softmax 处理：正面情感的概率更高（0.88）所以可以判定：<strong>模型判断这句话是正面情感</strong></p>
<p>可参考以下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleRNNClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_size, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleRNNClassifier, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 词嵌入层：将词ID转换为向量</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># RNN层：也可以换成 nn.LSTM 或 nn.GRU</span></span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 全连接分类层：从隐藏状态映射到类别</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x shape: [batch_size, seq_len]</span></span><br><span class="line">        x = <span class="variable language_">self</span>.embedding(x)            <span class="comment"># [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        </span><br><span class="line">        output, hidden = <span class="variable language_">self</span>.rnn(x)     <span class="comment"># output: [batch_size, seq_len, hidden_size]</span></span><br><span class="line">                                         <span class="comment"># hidden: [1, batch_size, hidden_size]</span></span><br><span class="line">        </span><br><span class="line">        last_hidden = hidden.squeeze(<span class="number">0</span>)  <span class="comment"># 去掉第一个维度 → [batch_size, hidden_size]</span></span><br><span class="line"></span><br><span class="line">        logits = <span class="variable language_">self</span>.fc(last_hidden)    <span class="comment"># [batch_size, num_classes]</span></span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="2-3-RNN的优势"><a href="#2-3-RNN的优势" class="headerlink" title="2.3 RNN的优势"></a>2.3 RNN的优势</h3><table>
<thead>
<tr>
<th>优势</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>状态记忆</td>
<td>能记住前面的信息并传递到后面</td>
</tr>
<tr>
<td>顺序敏感</td>
<td>非常适合处理时间或语义顺序强的数据</td>
</tr>
<tr>
<td>模型小</td>
<td>相比 Transformer，参数更少，适合轻量部署</td>
</tr>
<tr>
<td>在线预测</td>
<td>可以一边输入，一边预测，适合流数据处理</td>
</tr>
</tbody></table>
<h3 id="2-4-RNN存在的问题"><a href="#2-4-RNN存在的问题" class="headerlink" title="2.4 RNN存在的问题"></a>2.4 RNN存在的问题</h3><table>
<thead>
<tr>
<th>问题</th>
<th>描述</th>
<th>应对方案</th>
</tr>
</thead>
<tbody><tr>
<td>梯度消失 &#x2F; 爆炸</td>
<td>随序列变长，梯度难以传播</td>
<td>使用 LSTM&#x2F;GRU、梯度裁剪</td>
</tr>
<tr>
<td>长期依赖建模困难</td>
<td>只能记住近邻信息</td>
<td>LSTM 改进了这一问题</td>
</tr>
<tr>
<td>并行性差</td>
<td>每个时间步依赖前一时刻结果</td>
<td>用 Transformer 替代</td>
</tr>
<tr>
<td>训练慢</td>
<td>序列太长时效率低</td>
<td>使用注意力机制或 Transformer</td>
</tr>
</tbody></table>
<h3 id="2-5-RNN在当下的应用"><a href="#2-5-RNN在当下的应用" class="headerlink" title="2.5 RNN在当下的应用"></a>2.5 RNN在当下的应用</h3><p>RNN 是为了处理<strong>序列数据</strong>设计的网络，虽然被 Transformer 大量取代，但在一些轻量场景中依旧有用武之地。</p>
<h4 id="2-5-1-自然语言处理（部分任务）"><a href="#2-5-1-自然语言处理（部分任务）" class="headerlink" title="2.5.1 自然语言处理（部分任务）"></a>2.5.1 自然语言处理（部分任务）</h4><ul>
<li><strong>语音识别（ASR）系统中的声学模型</strong>：在边缘设备或小模型需求场景中，RNN（如GRU、LSTM）仍是主力。</li>
<li><strong>字符级文本生成</strong>：用于拼写建议、输入法候选词预测。</li>
<li><strong>简单的对话机器人或意图识别系统</strong>：轻量化需求场景中仍采用 RNN。</li>
</ul>
<h4 id="2-5-2-时间序列预测"><a href="#2-5-2-时间序列预测" class="headerlink" title="2.5.2 时间序列预测"></a>2.5.2 时间序列预测</h4><ul>
<li><strong>金融时间序列预测</strong>：如股票、加密货币价格预测等中，LSTM 仍被使用。</li>
<li><strong>传感器数据建模</strong>：如工业物联网设备状态预测、交通流量预测等。</li>
<li><strong>能耗预测</strong>：电力、水务公司对未来用量的预测任务中，RNN（尤其是双向LSTM）仍具有竞争力。</li>
</ul>
<h4 id="2-5-3-音频与语音"><a href="#2-5-3-音频与语音" class="headerlink" title="2.5.3 音频与语音"></a>2.5.3 音频与语音</h4><ul>
<li><strong>语音合成（TTS）</strong>：虽然主流是 Transformer，如 FastSpeech，但 RNN 如 Tacotron 系列仍被保留。</li>
<li><strong>音乐生成</strong>：在基于 MIDI 的音乐生成中 RNN 仍有用武之地。</li>
</ul>
<h2 id="三-Transformer"><a href="#三-Transformer" class="headerlink" title="三. Transformer"></a>三. Transformer</h2><h3 id="3-1-Transformer出现的背景"><a href="#3-1-Transformer出现的背景" class="headerlink" title="3.1 Transformer出现的背景"></a>3.1 Transformer出现的背景</h3><p>虽然 RNN&#x2F;LSTM 适合处理序列数据，但它们存在显著局限：</p>
<ol>
<li>序列处理是<strong>串行的</strong>，无法并行训练，效率低；</li>
<li><strong>长距离依赖建模能力差</strong>，随着序列长度增加，捕捉前面信息变得困难；</li>
<li>模型训练难度大，梯度传播路径长。</li>
</ol>
<p>2017 年，Google 提出的论文《Attention is All You Need》首次提出 <strong>Transformer 架构</strong>，抛弃 RNN 结构，完全基于注意力机制建模序列关系。它的主要创新包括：</p>
<ul>
<li>使用 <strong>Self-Attention</strong> 实现信息交互；</li>
<li>实现了 <strong>全序列并行计算</strong>；</li>
<li>架构对任务更具通用性和可扩展性。</li>
</ul>
<p>Transformer 一经推出即在机器翻译任务上超越传统 RNN 模型，并为后续一系列大型预训练模型（如 BERT、GPT）奠定基础。</p>
<h3 id="3-2-Transformer的优势"><a href="#3-2-Transformer的优势" class="headerlink" title="3.2 Transformer的优势"></a>3.2 Transformer的优势</h3><table>
<thead>
<tr>
<th>优势</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>并行计算能力</td>
<td>自注意力机制允许对整个序列同时处理，训练速度远快于 RNN（无序列依赖）。</td>
</tr>
<tr>
<td>长距离依赖建模</td>
<td>可以捕捉任意两个位置之间的关系，解决 RNN 难以建模长期依赖的问题。</td>
</tr>
<tr>
<td>表达能力强</td>
<td>多头注意力和深层堆叠结构使模型具有更强的特征提取和组合能力。</td>
</tr>
<tr>
<td>灵活结构</td>
<td>既可用于编码（如 BERT），也可用于解码（如 GPT），可自由组合应用场景。</td>
</tr>
<tr>
<td>可扩展性强</td>
<td>模型结构简单，易于扩展成大规模预训练模型（如 GPT-4、PaLM）。</td>
</tr>
<tr>
<td>通用性广泛</td>
<td>不仅适用于 NLP，还能用于图像、音频、多模态、生物信息等多领域任务。</td>
</tr>
<tr>
<td>迁移能力强</td>
<td>预训练模型可以微调到各种下游任务，极大降低任务训练成本。</td>
</tr>
</tbody></table>
<h3 id="3-3-Transformer的核心架构"><a href="#3-3-Transformer的核心架构" class="headerlink" title="3.3 Transformer的核心架构"></a>3.3 Transformer的核心架构</h3><p>论文地址: <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Transformer论文原文</a></p>
<p>Transformer的核心思想: 纯粹依靠<strong>注意力机制</strong>来处理序列数据。<strong>传统的注意力机制</strong>通常用在<strong>编码器-解码器</strong>架构中，<strong>解码器</strong>在生成每个输出时会”注意”<strong>编码器</strong>的不同位置，这是<strong>跨序列的注意力</strong>。而<strong>自注意力</strong>是<strong>序列内部</strong>的注意力机制，它让序列中的每个位置都能够与同一序列中的所有其他位置建立直接联系。比如在处理句子”The cat sat on the mat”时，自注意力让”cat”这个词不仅能看到自己，还能直接看到”The”、“sat”、“on”、“the”、”mat”等所有其他词，并根据语义相关性给它们分配不同的权重。</p>
<p>核心架构图:</p>
<p><img src="/images/image-20250707161014830.png" alt="image-20250707161014830"></p>
<p>该架构由编码器(Encoder)与解码器( Decoder)组成，左侧为Encoder，右侧为Decoder。每个编码器由1个位置编码层(图中的Positional Encoding)与N个编码层( Encoder Layer )组成, 每个解码器由1个位置编码层与N个解码层(Decoder Layer )以及1个以Sotfmax为激活函数的全连接层组成。第t个编码层的输入是t-1个编码层的输出，解码层同理。 </p>
<h4 id="编码器的结构"><a href="#编码器的结构" class="headerlink" title="编码器的结构:"></a>编码器的结构:</h4><h5 id="Embeddings-Projections（嵌入-投影层）"><a href="#Embeddings-Projections（嵌入-投影层）" class="headerlink" title="Embeddings&#x2F;Projections（嵌入&#x2F;投影层）:"></a><strong>Embeddings&#x2F;Projections（嵌入&#x2F;投影层）</strong>:</h5><p>将输入的单词（或 token）转换成数字向量（比如 “猫” → [0.2, -0.5, 0.7…]）。</p>
<h5 id="Positional-Encoding-位置编码"><a href="#Positional-Encoding-位置编码" class="headerlink" title="Positional Encoding(位置编码) :"></a>Positional Encoding(位置编码) :</h5><p>Transformer 本身没有循环或卷积结构，无法直接感知序列顺序，因此需要显式地注入位置信息。</p>
<p>位置编码可参考以下的计算公式:</p>
<p><img src="/images/image-20250708000309893.png" alt="image-20250708000309893"></p>
<p><code>pos</code> 表示当前位置（第几个词）,<code>i</code> 表示词向量的第 i 维, <code>d</code> 是词向量总维度（如 512）</p>
<h5 id="Multi-Head-Attention-多头注意力"><a href="#Multi-Head-Attention-多头注意力" class="headerlink" title="Multi_Head Attention (多头注意力) :"></a><strong>Multi_Head Attention (多头注意力)</strong> :</h5><ol>
<li><p>输入变换：输入的向量经过变换分别得到查询（Query）、键（Key）和值（Value)矩阵。</p>
<p><strong>查询（Query）：</strong> 指的是查询的范围，自主提示，即主观意识的特征向量</p>
<p><strong>键（Key）：</strong> 指的是被比对的项，非自主提示，即物体的突出特征信息向量</p>
<p><strong>值（Value） ：</strong>  则是代表物体本身的特征向量，通常和Key成对出现</p>
<p><img src="/images/image-20250708012007135.png" alt="image-20250708012007135"></p>
<p>注意力机制是通过<strong>Query</strong>与<strong>Key</strong>的注意力汇聚（给定一个 <strong>Query</strong>，计算<strong>Query</strong>与 <strong>Key</strong>的相关性，然后根据<strong>Query</strong>与<strong>Key</strong>的相关性去找到最合适的 <strong>Value</strong>）实现对<strong>Value</strong>的注意力权重分配，生成最终的输出结果。</p>
<p>举个例子: </p>
<ol>
<li><p>当你用上淘宝购物时，你会敲入一句关键词（比如：显瘦），这个就是<strong>Query</strong>。</p>
</li>
<li><p>搜索系统会根据关键词这个去查找一系列相关的<strong>Key</strong>（商品名称、图片）。</p>
</li>
<li><p>最后系统会将相应的 <strong>Value</strong> （具体的衣服）返回给你。</p>
</li>
</ol>
</li>
<li><p>多头：即定义多组W，生成多组Q、K、V，每个头具有不同的线性变换参数。</p>
</li>
<li><p>注意力计算：对于每个头，都执行一次缩放点积注意力运算。具体来说，计算查询和键的点积，经过缩放、加上偏置后，使用softmax函数得到注意力权重。这些权重用于加权值矩阵，生成加权和作为每个头的输出。</p>
<p>计算公式参照以下内容:</p>
<p><img src="/images/image-20250707234854789.png" alt="image-20250707234854789"></p>
</li>
<li><p>拼接与融合：将所有头的输出矩阵拼接在一起，形成一个长向量。然后，然后将它们乘以一个额外的权重矩阵 **<img src="https://ucc.alicdn.com/pic/developer-ecology/mkmroptvcwdrc_0a66413fc59146f1a54cf8e2ed98e28c.png" alt="image">**做一次线性变换降维，得到最终的多头注意力输出。</p>
</li>
</ol>
<h5 id="Add-Norm-残差连接与层归一化"><a href="#Add-Norm-残差连接与层归一化" class="headerlink" title="Add &amp; Norm (残差连接与层归一化) :"></a>Add &amp; Norm (残差连接与层归一化) :</h5><p>残差连接: 残差连接 &#x3D; 原始输入 + 子层输出，目的在于缓解梯度消失。举个例子如下所示:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">原始输入为:x = [1.2, 0.5, -0.7, ..., 0.3]  # 维度比如是 512</span><br><span class="line">经过注意力层处理之后: SubLayer(x) = [0.9, -0.4, 0.6, ..., 0.2]</span><br><span class="line">加上原始输入：output = x + SubLayer(x) = [2.1, 0.1, -0.1, ..., 0.5]</span><br></pre></td></tr></table></figure>



<p>层归一化（Layer Normalization）: 它会对一个样本的所有特征做归一化，稳定网络训练减少梯度爆炸&#x2F;消失风险，加速收敛。</p>
<p> 计算公式为:<img src="/images/image-20250708002315940.png" alt="image-20250708002315940"></p>
<p>其中mean(x)表示求平均值, std(x)表示标准差。举个例子:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = [2.0, 4.0, 6.0, 8.0]</span><br><span class="line">经计算mean(x) = 5，std(x) = 2.236</span><br><span class="line">逐个计算归一化的值为: -1.34,-0.45,0.45,1.34</span><br><span class="line">经过这个归一化,我们发现：</span><br><span class="line">(1)所有值都围绕 0 对称了（均值为 0）</span><br><span class="line">(2)数据范围被缩放了（标准差为 1）</span><br><span class="line">(3)保留了原来的相对大小关系，但更适合训练模型</span><br></pre></td></tr></table></figure>



<h5 id="Feed-Forward-前馈神经网络"><a href="#Feed-Forward-前馈神经网络" class="headerlink" title="Feed Forward (前馈神经网络) :"></a>Feed Forward (前馈神经网络) :</h5><p> 每个位置的表示会通过一个两层全连接网络（含非线性激活，如ReLU）进行进一步变换。</p>
<p>可参照以下公式:</p>
<p><img src="/images/image-20250708003319851.png" alt="image-20250708003319851"></p>
<h4 id="解码器的结构"><a href="#解码器的结构" class="headerlink" title="解码器的结构:"></a>解码器的结构:</h4><p>解码器比编码器多了Masked Multi_Head Attention与Multi_Head Attention(该部分与编码器略有不同)</p>
<h5 id="Masked-Multi-Head-Attention-遮盖的多头注意力层"><a href="#Masked-Multi-Head-Attention-遮盖的多头注意力层" class="headerlink" title="Masked Multi_Head Attention (遮盖的多头注意力层):"></a>Masked Multi_Head Attention (遮盖的多头注意力层):</h5><p>遮盖的意义是为了将未来的信息进行掩盖 。保证解码时生成第 t 个词时，只用到前面 1~t 的信息，不能“偷看答案”。</p>
<p>Transformer用一个掩码矩阵(mask)来实现。</p>
<p>对长度为 <code>seq_len = 5</code> 的序列，mask 是一个上三角矩阵(0表示遮盖)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[1, 0, 0, 0, 0],</span><br><span class="line"> [1, 1, 0, 0, 0],</span><br><span class="line"> [1, 1, 1, 0, 0],</span><br><span class="line"> [1, 1, 1, 1, 0],</span><br><span class="line"> [1, 1, 1, 1, 1]]</span><br></pre></td></tr></table></figure>



<h5 id="Multi-Head-Attention-交互注意力层"><a href="#Multi-Head-Attention-交互注意力层" class="headerlink" title="Multi_Head Attention (交互注意力层) :"></a>Multi_Head Attention (交互注意力层) :</h5><p>在编码器中Q,K,V的计算方式为:</p>
<p><img src="/images/image-20250708012227737.png" alt="image-20250708012227737"></p>
<p>而在解码器中Q计算方式有所不同 :</p>
<p><img src="/images/image-20250708012300834.png" alt="image-20250708012300834"></p>
<h3 id="3-4-Transformer存在的问题"><a href="#3-4-Transformer存在的问题" class="headerlink" title="3.4 Transformer存在的问题"></a>3.4 Transformer存在的问题</h3><table>
<thead>
<tr>
<th>问题</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>计算复杂度高</td>
<td>Self-Attention 的计算量为 O(n^2)，序列越长，资源消耗越大。</td>
</tr>
<tr>
<td>内存占用大</td>
<td>多头注意力和大层数堆叠导致显存需求大，训练和推理成本高。</td>
</tr>
<tr>
<td>对位置信息不敏感</td>
<td>需要显式加入位置编码，才能建模序列顺序，而不像 RNN 自然具备时间顺序感。</td>
</tr>
<tr>
<td>训练依赖大量数据</td>
<td>从零训练效果好需要大规模语料和算力，否则容易过拟合或收敛慢。</td>
</tr>
<tr>
<td>可解释性差</td>
<td>虽有 Attention 可视化手段，但整体模型仍为黑盒，难以理解内部推理机制。</td>
</tr>
<tr>
<td>推理延迟高</td>
<td>虽然训练可以并行，<strong>自回归式生成</strong>（如 GPT）在推理时仍需一步步计算，速度慢。</td>
</tr>
<tr>
<td>小模型表现不稳定</td>
<td>在小数据、小模型场景下效果不如 RNN，容易欠拟合或不收敛。</td>
</tr>
</tbody></table>
<h3 id="3-5-Transformer在当下的应用"><a href="#3-5-Transformer在当下的应用" class="headerlink" title="3.5 Transformer在当下的应用"></a>3.5 Transformer在当下的应用</h3><p>Transformer 已成为 AI 领域的主流架构，广泛应用于自然语言处理、计算机视觉、多模态系统等领域，尤其在大模型时代扮演核心角色。</p>
<h4 id="3-5-1-自然语言处理（NLP）"><a href="#3-5-1-自然语言处理（NLP）" class="headerlink" title="3.5.1 自然语言处理（NLP）"></a>3.5.1 自然语言处理（NLP）</h4><ul>
<li><strong>大语言模型（LLM）</strong>：GPT 系列（GPT-2&#x2F;3&#x2F;4&#x2F;ChatGPT）、BERT、T5 等均基于 Transformer 架构。</li>
<li><strong>机器翻译</strong>：Transformer 最初即用于翻译任务，已完全替代传统 RNN 模型（如 Google Translate）。</li>
<li><strong>信息抽取、问答系统、文本分类</strong>：BERT 系列被广泛用于下游任务。</li>
<li><strong>语义搜索与对话系统</strong>：如语义检索、FAQ系统、智能客服中的核心模块。</li>
</ul>
<h4 id="3-5-2-计算机视觉（CV）"><a href="#3-5-2-计算机视觉（CV）" class="headerlink" title="3.5.2 计算机视觉（CV）"></a>3.5.2 计算机视觉（CV）</h4><ul>
<li><strong>图像分类与识别</strong>：Vision Transformer（ViT）直接在图像块上应用 Self-Attention。</li>
<li><strong>目标检测与图像分割</strong>：DETR、Mask2Former 等引入 Transformer 编码空间关系。</li>
<li><strong>医学影像处理</strong>：在脑部扫描、X 光图像分析中，Transformer 模型逐渐替代传统 CNN。</li>
</ul>
<h4 id="3-5-3-多模态学习"><a href="#3-5-3-多模态学习" class="headerlink" title="3.5.3 多模态学习"></a>3.5.3 多模态学习</h4><ul>
<li><strong>图文匹配与生成</strong>：如 OpenAI 的 CLIP、DALL·E，将文本与图像共同建模。</li>
<li><strong>视频生成与理解</strong>：Sora、Video-Transformer 等用于视频问答、生成、动作识别。</li>
<li><strong>跨模态检索与交互</strong>：用于多模态搜索引擎、VQA（视觉问答）等。</li>
</ul>
<h4 id="3-5-4-语音与音频处理"><a href="#3-5-4-语音与音频处理" class="headerlink" title="3.5.4 语音与音频处理"></a>3.5.4 语音与音频处理</h4><ul>
<li><strong>语音识别（ASR）</strong>：如 Whisper 模型，使用 Transformer 捕捉长距离上下文。</li>
<li><strong>语音合成（TTS）</strong>：FastSpeech 系列引入 Transformer 提升语音自然度与推理效率。</li>
<li><strong>音乐生成与伴奏生成</strong>：Transformer 被用于生成 MIDI 音符序列或和声结构。</li>
</ul>
<h4 id="3-5-5-强化学习与控制系统"><a href="#3-5-5-强化学习与控制系统" class="headerlink" title="3.5.5 强化学习与控制系统"></a>3.5.5 强化学习与控制系统</h4><ul>
<li><strong>Decision Transformer</strong>：将序列建模思想引入强化学习，实现无模型策略优化。</li>
<li><strong>游戏 AI</strong>：在围棋、Minecraft、Atari 等任务中，Transformer 增强策略学习和世界建模。</li>
</ul>
<h2 id="四-BERT"><a href="#四-BERT" class="headerlink" title="四. BERT"></a>四. BERT</h2><h3 id="4-1-BERT出现的背景"><a href="#4-1-BERT出现的背景" class="headerlink" title="4.1 BERT出现的背景"></a>4.1 BERT出现的背景</h3><p>在 Transformer 被提出之后（2017年），虽然它已经在机器翻译等任务中表现出色，但 <strong>多数 NLP 任务依然采用“从零训练”或“静态词向量（如 word2vec、GloVe）+任务特定模型”</strong> 的方法。这种做法存在以下问题：</p>
<ol>
<li><strong>无法共享知识</strong>：每个任务都需要单独训练模型，数据和计算资源浪费严重；</li>
<li><strong>词向量静态</strong>：传统词向量无法根据上下文动态调整语义，”bank” 一词在“河岸”与“银行”中的意思是一样的；</li>
<li><strong>上下文建模能力弱</strong>：传统模型通常只能从左到右或右到左理解句子，不能双向建模完整上下文。</li>
</ol>
<p>于是，Google 在 2018 年提出了 <strong>BERT（Bidirectional Encoder Representations from Transformers）</strong>，其主要创新是：</p>
<ul>
<li><strong>引入了“预训练 + 微调”框架</strong>；</li>
<li><strong>采用双向 Transformer 编码器</strong>；</li>
<li><strong>设计了两个预训练任务（MLM 和 NSP）</strong> 来帮助模型学习语义知识。</li>
</ul>
<p>BERT 一经推出，便刷新了多项 NLP 任务的性能记录，标志着 <strong>预训练语言模型时代的到来</strong>。</p>
<h3 id="4-2-BERT的核心架构"><a href="#4-2-BERT的核心架构" class="headerlink" title="4.2 BERT的核心架构"></a>4.2 BERT的核心架构</h3><p><strong>BERT 只用了 Encoder</strong>，完全舍弃 Decoder，它不是生成模型，而是一个<strong>双向理解模型</strong>。</p>
<p><img src="/images/image-20250708013556341.png" alt="image-20250708013556341"></p>
<p>接下来具体介绍下<strong>Tokenizer, Embedding, 输出层(MLM,NSP)</strong>。</p>
<h4 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h4><p>原始文本首先会被送入分词器(Tokenizer)进行切分处理，生成Token序列。这一过程通常包含文本标准化(如转为小写)、标点符号过滤以及词语切分等步骤。举个例子:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入句子：I love deep learning</span><br><span class="line">Token 化：[&#x27;[CLS]&#x27;, &#x27;i&#x27;, &#x27;love&#x27;, &#x27;deep&#x27;, &#x27;learn&#x27;, &#x27;##ing&#x27;, &#x27;[SEP]&#x27;]</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li><p><code>[CLS]</code>：句首分类符号（用于分类等任务）</p>
</li>
<li><p><code>[SEP]</code>：句子之间的分隔符</p>
</li>
<li><p><code>##ing</code>：子词表示，这是 WordPiece 的规则，表示它是 <code>learn</code> 的一部分</p>
</li>
</ul>
<p>接下来会将将每个 token 映射为 ID（索引）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;[CLS]&#x27;, &#x27;i&#x27;, &#x27;love&#x27;, &#x27;deep&#x27;, &#x27;learn&#x27;, &#x27;##ing&#x27;, &#x27;[SEP]&#x27;]</span><br><span class="line">[101, 1045, 2293, 2784, 4553, 2075, 102]</span><br><span class="line">这些数字是 BERT 词表中的索引。</span><br></pre></td></tr></table></figure>



<h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><p>对于每个 token，BERT 会创建以下三个向量并相加：</p>
<table>
<thead>
<tr>
<th>向量类型</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Token Embedding</strong></td>
<td>词本身的词向量（WordPiece 生成）</td>
</tr>
<tr>
<td><strong>Position Embedding</strong></td>
<td>每个词在句子中的<strong>位置编号</strong>对应的向量（例如第0位、第1位……）</td>
</tr>
<tr>
<td><strong>Segment Embedding</strong></td>
<td>表示这个词属于哪一个句子（A → 0，B → 1）</td>
</tr>
</tbody></table>
<p>举个例子:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">句子A: &quot;I love NLP&quot;</span><br><span class="line">句子B: &quot;It is great&quot;</span><br><span class="line"></span><br><span class="line">输入：</span><br><span class="line">[CLS] I love NLP [SEP] It is great [SEP]</span><br><span class="line"></span><br><span class="line">→ token_ids:     [101, 1045, 2293, 17953, 102, 2009, 2003, 2307, 102]</span><br><span class="line">→ position_ids:  [  0,    1,    2,     3,   4,    5,    6,    7,   8]</span><br><span class="line">→ segment_ids:   [  0,    0,    0,     0,   0,    1,    1,    1,   1]</span><br></pre></td></tr></table></figure>

<p><strong>embedding &#x3D; token_embed + position_embed + segment_embed</strong></p>
<h4 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h4><p><img src="/images/image-20250708015501382.png" alt="image-20250708015501382"></p>
<h4 id="输出层-MLM-NSP"><a href="#输出层-MLM-NSP" class="headerlink" title="输出层(MLM,NSP)"></a>输出层(MLM,NSP)</h4><h5 id="MLM-MaskedLanguageModel"><a href="#MLM-MaskedLanguageModel" class="headerlink" title="MLM (MaskedLanguageModel)"></a>MLM (MaskedLanguageModel)</h5><p>目标是给定一个句子，随机遮盖其中一部分单词，要求模型根据上下文预测这些被遮盖的词。</p>
<p>特点:</p>
<ul>
<li>传统语言模型是“从左到右”或“从右到左”预测，只能使用部分上下文；</li>
<li>MLM 是 <strong>双向建模</strong>，同时看到左右文，学习更丰富的上下文依赖；</li>
<li>15% 掩码策略可以避免模型过度依赖某一部分上下文，增强泛化。</li>
</ul>
<p>具体过程:</p>
<p>随机选取输入序列中 <strong>15% 的 token</strong>；对这些 token，按照以下概率进行处理：</p>
<ul>
<li>80% → 替换成 <code>[MASK]</code>（如“我[mask]吃苹果”）</li>
<li>10% → 替换成一个随机词（如“我狗吃苹果”）</li>
<li>10% → 保留原词（如“我爱吃苹果”）</li>
</ul>
<p>模型接收这个“被遮挡”的句子作为输入；<strong>输出层是一个 Softmax 分类器</strong>，在词汇表中选择最可能的词填入 Mask 位置。</p>
<h5 id="NSP-NextSentence-Prediction"><a href="#NSP-NextSentence-Prediction" class="headerlink" title="NSP (NextSentence Prediction)"></a>NSP (NextSentence Prediction)</h5><p>目标是给定两个句子 A 和 B，判断 B 是不是 A 的下一句。</p>
<p>特点:</p>
<ul>
<li>帮助模型学习<strong>句子之间的逻辑关系</strong>，如因果、承接、转折等；</li>
<li>有助于下游任务如问答（QA）、自然语言推理（NLI）；</li>
<li>建立跨句子的语义理解能力，不只是 token 层面建模。</li>
</ul>
<p>具体过程:</p>
<p>构建句子对输入：<code>[CLS] 句子A [SEP] 句子B [SEP]</code>数据标签分两种：</p>
<ul>
<li>50% 是真实的下一句（正例）</li>
<li>50% 是随机采样的句子（负例）</li>
</ul>
<p>将 <code>[CLS]</code> 位置的输出向量送入一个二分类器，判断是否为“下一句”。</p>
<h3 id="4-3-BERT的优势"><a href="#4-3-BERT的优势" class="headerlink" title="4.3 BERT的优势"></a>4.3 BERT的优势</h3><table>
<thead>
<tr>
<th>优势</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>双向建模能力</td>
<td>使用 Masked LM，可以同时看到左侧和右侧上下文，更精准地理解句子含义。</td>
</tr>
<tr>
<td>预训练+微调范式</td>
<td>预训练后可迁移到多种下游任务，大幅减少标注数据需求。</td>
</tr>
<tr>
<td>上下文感知词向量</td>
<td>同一个词在不同上下文中会有不同表示，克服静态词向量表达不清的问题。</td>
</tr>
<tr>
<td>开源生态成熟</td>
<td>HuggingFace 等库提供丰富模型，易于调用、微调与集成。</td>
</tr>
<tr>
<td>表现强大</td>
<td>在 GLUE、SQuAD、NER、文本分类等多种任务中均刷新当时最优表现。</td>
</tr>
</tbody></table>
<h3 id="4-4-BERT存在的问题"><a href="#4-4-BERT存在的问题" class="headerlink" title="4.4 BERT存在的问题"></a>4.4 BERT存在的问题</h3><table>
<thead>
<tr>
<th>问题</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>推理速度慢</td>
<td>输入必须同时处理完整序列，不能像 GPT 那样逐字生成，部署成本高。</td>
</tr>
<tr>
<td>预训练成本高</td>
<td>需要数百万步的训练，依赖 TPUs 或大型 GPU 集群。</td>
</tr>
<tr>
<td>无法自然生成文本</td>
<td>作为 Encoder-only 架构，BERT 不能用来进行语言生成任务（如写文章）。</td>
</tr>
<tr>
<td>句子级建模局限</td>
<td>NSP（Next Sentence Prediction）任务被质疑效果有限，BERT对长文本建模弱。</td>
</tr>
<tr>
<td>结构固定</td>
<td>原版 BERT 是非自回归模型，无法灵活处理变长输入或交互式任务。</td>
</tr>
</tbody></table>
<h3 id="4-5-BERT在当下的应用"><a href="#4-5-BERT在当下的应用" class="headerlink" title="4.5 BERT在当下的应用"></a>4.5 BERT在当下的应用</h3><table>
<thead>
<tr>
<th>应用领域</th>
<th>实际应用</th>
</tr>
</thead>
<tbody><tr>
<td>搜索与排序系统</td>
<td>如 Google 搜索引擎中集成 BERT，用于理解查询意图与内容匹配。</td>
</tr>
<tr>
<td>智能问答</td>
<td>应用于 FAQ 系统、在线客服、医学&#x2F;金融问答等场景。</td>
</tr>
<tr>
<td>文本理解类任务</td>
<td>情感分析、文本分类、命名实体识别（NER）、关系抽取等。</td>
</tr>
<tr>
<td>文档匹配与推荐</td>
<td>在推荐系统中分析用户评论、内容标签，提高匹配效果。</td>
</tr>
<tr>
<td>政务、司法系统</td>
<td>用于合同理解、法律条文比对、政策问答等领域的自动文本分析。</td>
</tr>
<tr>
<td>模型微调底座</td>
<td>被广泛用于下游模型微调的起点，如 RoBERTa、ALBERT、TinyBERT 等均由 BERT 演化而来。</td>
</tr>
</tbody></table>
<h2 id="五-GPT-生成式预训练"><a href="#五-GPT-生成式预训练" class="headerlink" title="五. GPT (生成式预训练)"></a>五. GPT (生成式预训练)</h2><h3 id="5-1-GPT出现的背景"><a href="#5-1-GPT出现的背景" class="headerlink" title="5.1 GPT出现的背景"></a>5.1 GPT出现的背景</h3><p>BERT 的提出解决了很多理解类任务的问题，但它作为 <strong>Encoder-only 模型</strong>，并不适合用于<strong>文本生成任务</strong>。而在生成式任务中，如：</p>
<ul>
<li>写文章、自动续写、</li>
<li>对话系统、代码生成、</li>
<li>多轮问答和创作等，</li>
</ul>
<p>我们需要的是一种 <strong>自回归语言模型（Auto-regressive LM）</strong> —— 能够基于已知内容一步步“预测下一个词”的结构。</p>
<p>因此，OpenAI 提出了 GPT（Generative Pre-trained Transformer），最初在 2018 年发布 GPT-1，随后：</p>
<ul>
<li>2019 年 GPT-2 开始引起广泛关注；</li>
<li>2020 年 GPT-3 横空出世，凭借 1750 亿参数登顶 NLP 高峰；</li>
<li>2022 年开始的 ChatGPT（基于 GPT-3.5 &#x2F; GPT-4）进一步展示了其对话与创作能力。</li>
</ul>
<p>GPT 模型本质是基于 <strong>Transformer 的 Decoder 部分结构</strong>，采用 <strong>自回归训练目标（预测下一个词）</strong>，自然适用于生成任务。</p>
<h3 id="5-2-GPT的核心架构"><a href="#5-2-GPT的核心架构" class="headerlink" title="5.2 GPT的核心架构"></a>5.2 GPT的核心架构</h3><p><img src="/images/image-20250708013711423.png" alt="image-20250708013711423"></p>
<h4 id="自回归生成"><a href="#自回归生成" class="headerlink" title="自回归生成"></a>自回归生成</h4><p>它的基本思想是: <strong>当前输出依赖于之前的所有输出</strong>。</p>
<p>整个句子的生成是每个 token 的条件概率连乘:</p>
<p><img src="/images/image-20250708023132171.png" alt="image-20250708023132171"></p>
<p>在语言建模中，就是：</p>
<ul>
<li>模型每次预测一个单词或一个 token（词片段）</li>
<li>这个 token 的预测是基于“之前已经生成的所有 token”</li>
<li>然后再把这个新生成的 token 作为输入的一部分，继续生成下一个 token</li>
</ul>
<p>举个例子:</p>
<p><strong>输入文本编码</strong>：</p>
<ul>
<li>比如我们输入开头“今天是”</li>
<li>被分成多个 token，如：[“今”, “天”, “是”]</li>
</ul>
<p><strong>自注意力掩码机制（Mask）</strong>：</p>
<ul>
<li>为了确保模型<strong>只能看到前面的词</strong>，不能看到“未来”内容（防止泄露）</li>
<li>使用了 <strong>masked self-attention</strong>（掩码自注意力,前文的Transformer有提到）</li>
</ul>
<p><strong>逐个生成 token</strong>：</p>
<ul>
<li>首先基于输入“今天是”，生成下一个 token，例如“晴”</li>
<li>然后组合成“今天是晴”，再输入到模型中继续生成下一个 token，例如“天”</li>
<li>如此循环，直到生成终止符或达到最大长度</li>
</ul>
<h3 id="5-3-GPT的优势"><a href="#5-3-GPT的优势" class="headerlink" title="5.3 GPT的优势"></a>5.3 GPT的优势</h3><table>
<thead>
<tr>
<th>优势</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>自然语言生成能力强</td>
<td>自回归建模，擅长连续写作、对话、总结、翻译等生成式任务。</td>
</tr>
<tr>
<td>预训练语义丰富</td>
<td>使用海量语料训练，具备丰富的世界知识、语义常识和语言理解能力。</td>
</tr>
<tr>
<td>通用性极强</td>
<td>一个模型可迁移至问答、摘要、翻译、代码生成、诗歌创作等多种任务。</td>
</tr>
<tr>
<td>可对话与交互</td>
<td>通过微调或强化学习（如 RLHF）适配多轮对话，构建 ChatGPT 类产品。</td>
</tr>
<tr>
<td>架构简洁</td>
<td>仅基于 Transformer 的 Decoder 结构，训练和推理流程一致，便于部署。</td>
</tr>
<tr>
<td>高扩展性</td>
<td>GPT 架构容易通过增加层数、参数量扩展至大模型（GPT-3、GPT-4 等）。</td>
</tr>
</tbody></table>
<h3 id="5-4-GPT存在的问题"><a href="#5-4-GPT存在的问题" class="headerlink" title="5.4 GPT存在的问题"></a>5.4 GPT存在的问题</h3><table>
<thead>
<tr>
<th>问题</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>上下文窗口限制</td>
<td>模型一次只能看到有限长度的上下文，早期 GPT 模型窗口长度较短。</td>
</tr>
<tr>
<td>内容准确性问题</td>
<td>模型容易“一本正经地胡说八道”（hallucination），生成不真实或不合理内容。</td>
</tr>
<tr>
<td>训练资源极高</td>
<td>GPT-3&#x2F;GPT-4 等需巨量数据和超大算力，训练成本高昂，非普通组织可承担。</td>
</tr>
<tr>
<td>长文本推理困难</td>
<td>虽然能生成长文本，但推理链条复杂时，常出现逻辑跳跃或语义漂移。</td>
</tr>
<tr>
<td>缺乏世界动态知识</td>
<td>模型无法实时获取新知识，生成内容有滞后性（除非接入搜索引擎等外部工具）。</td>
</tr>
<tr>
<td>安全性与伦理问题</td>
<td>有可能生成有害内容、偏见信息、虚假陈述，需要对输出内容进行严格对齐与控制。</td>
</tr>
</tbody></table>
<h3 id="5-5-GPT在当下的应用"><a href="#5-5-GPT在当下的应用" class="headerlink" title="5.5 GPT在当下的应用"></a>5.5 GPT在当下的应用</h3><table>
<thead>
<tr>
<th>应用领域</th>
<th>实际应用</th>
</tr>
</thead>
<tbody><tr>
<td>智能对话与客服</td>
<td>ChatGPT、Copilot Chat、企业客服机器人，适配多轮交互式对话任务。</td>
</tr>
<tr>
<td>文本创作与辅助写作</td>
<td>撰写邮件、改写句子、写小说、写诗歌、文案生成、写 PPT 提纲等。</td>
</tr>
<tr>
<td>文档总结与理解</td>
<td>会议纪要生成、文档摘要、法律文本解读、医学文献分析等。</td>
</tr>
<tr>
<td>翻译与语言处理</td>
<td>中英互译、风格变换（正式&#x2F;口语化）、语言校对与润色。</td>
</tr>
<tr>
<td>编程辅助</td>
<td>Copilot、Code Interpreter、代码补全、调试建议、正则表达式生成。</td>
</tr>
<tr>
<td>教育与答疑</td>
<td>辅助教学、作业批改、数学题解答、编程教学、自适应学习系统。</td>
</tr>
<tr>
<td>多模态模型接入</td>
<td>接入图像、语音、视频等输入后，作为视觉问答、AI 助理的语言核心模块。</td>
</tr>
</tbody></table>
<h2 id="六-常见神经网络模型对比表"><a href="#六-常见神经网络模型对比表" class="headerlink" title="六. 常见神经网络模型对比表"></a>六. 常见神经网络模型对比表</h2><table>
<thead>
<tr>
<th>维度</th>
<th>CNN</th>
<th>RNN</th>
<th>Transformer</th>
<th>BERT</th>
<th>GPT</th>
</tr>
</thead>
<tbody><tr>
<td>基础结构</td>
<td>卷积层 +    池化层</td>
<td>循环单元（LSTM&#x2F;GRU）</td>
<td>自注意力机制 + 前馈网络</td>
<td>Transformer Encoder（双向）</td>
<td>Transformer Decoder    （单向）</td>
</tr>
<tr>
<td>输入依赖方式</td>
<td>局部感知、无顺序依赖</td>
<td>按时间序列，前后依赖强</td>
<td>使用位置编码并行处理</td>
<td>双向编码（上下文同时建模）</td>
<td>单向生成（左到右）</td>
</tr>
<tr>
<td>注意力机制</td>
<td>无</td>
<td>无（传统RNN），LSTM&#x2F;GRU部分记忆</td>
<td>多头注意力机制（Multi-head Attention）</td>
<td>多头双向注意力</td>
<td>多头单向注意力（Mask Attention）</td>
</tr>
<tr>
<td>预训练任务</td>
<td>无</td>
<td>无</td>
<td>依赖具体任务</td>
<td>MLM + NSP</td>
<td>自回归语言建模（Next Token Prediction）</td>
</tr>
<tr>
<td>并行能力</td>
<td>强（卷积操作可并行）</td>
<td>弱（时间步依赖导致不能并行）</td>
<td>强（可完全并行）</td>
<td>强（训练时可并行）</td>
<td>强（训练时并行，生成时逐步）</td>
</tr>
<tr>
<td>表达能力</td>
<td>擅长局部特征提取</td>
<td>擅长处理序列依赖</td>
<td>处理长距离依赖能力强</td>
<td>语言理解能力强</td>
<td>文本生成能力强</td>
</tr>
<tr>
<td>应用场景</td>
<td>图像识别、文本分类等</td>
<td>语音识别、时间序列预测</td>
<td>机器翻译、问答、对话系统等</td>
<td>文本分类、问答、句子匹配等</td>
<td>对话系统、写作、代码生成、续写等</td>
</tr>
<tr>
<td>优点</td>
<td>高效、参数少、稳定</td>
<td>能捕捉时序信息</td>
<td>训练快、长依赖处理能力强</td>
<td>上下文理解强，预训练通用性好</td>
<td>可生成高质量文本，泛化能力强</td>
</tr>
<tr>
<td>缺点</td>
<td>缺乏全局依赖建模能力</td>
<td>长序列训练慢，梯度消失问题</td>
<td>参数多，训练资源需求大</td>
<td>无法生成文本，推理时需全序列输入</td>
<td>生成效率相对低，信息获取是单向的</td>
</tr>
</tbody></table>

        </article>
        <section class="post-near">
            <ul>
                
                    <li>上一篇: <a href="/2025/07/11/RAG%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%AE%B9%E5%99%A8%E5%8C%96%E5%AE%9E%E8%B7%B5/">RAG智能问答系统容器化实践</a></li>
                
                
                    <li>下一篇: <a href="/2025/07/05/docker/">docker学习笔记</a></li>
                
            </ul>
        </section>
        
            <section class="post-tags">
            <a class="-none-link" href="/tags/%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/" rel="tag">底层架构</a>
            </section>
        
    
        <section class="post-author">
        
            <figure class="author-avatar">
                <img src="https://sdn.geekzu.org/avatar/d22eb460ecab37fcd7205e6a3c55c228?s=200&r=X&d=" alt="hduer" />
            </figure>
        
            <div class="author-info">
                <h4>hduer</h4>
                <p>一个热爱生活的hduer</p>
            </div>
        </section>
    
    </div>
</main>

    <footer>
    <div class="buttons">
        <button class="to-top" href="#"></button>
    </div>
    <div class="wrap min">
        <section class="widget">
            <div class="row">
                <div class="col-m-4">
                    <h3 class="title-recent">最新文章：</h3>
                    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/07/21/leetcode-Day3/">leetcode-Day3</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/07/20/leetcode-Day2/">leetcode-Day2</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/07/19/leetcode-Day1/">leetcode-Day1</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/07/11/RAG%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%AE%B9%E5%99%A8%E5%8C%96%E5%AE%9E%E8%B7%B5/">RAG智能问答系统容器化实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/07/08/transformer/">Transformer学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/07/05/docker/">docker学习笔记</a></li></ul>
                </div>
                <div class="col-m-4">
                    <h3 class="title-date">时光机：</h3>
                    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li></ul>
                </div>
                <div class="col-m-4">
                    <h3 class="title-tags">标签云：</h3>
                    <a href="/tags/Dify/" style="font-size: 10px;">Dify</a> <a href="/tags/LeetCode/" style="font-size: 20px;">LeetCode</a> <a href="/tags/Leetcode/" style="font-size: 10px;">Leetcode</a> <a href="/tags/docker/" style="font-size: 20px;">docker</a> <a href="/tags/prompt/" style="font-size: 10px;">prompt</a> <a href="/tags/%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/" style="font-size: 10px;">底层架构</a>
                </div>
            </div>
        </section>
        <section class="sub-footer">
            <p>© 2025 <a href="/">Hduer</a>. All Rights Reserved. Theme By <a href="https://github.com/Dreamer-Paul/Hingle" target="_blank" rel="nofollow">Hingle</a>.</p>
        </section>
    </div>
</footer>


<script src="/static/kico.js"></script>
<script src="/static/hingle.js"></script>


<script>var hingle = new Paul_Hingle({"copyright":true,"night":true});</script>

  </body>
</html>
